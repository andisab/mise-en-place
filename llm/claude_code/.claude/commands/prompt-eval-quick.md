# Quick Prompt Evaluation

Rapidly evaluate and improve your prompts using key Claude 4 best practices.

## Core Evaluation Criteria

### 1. **Clarity & Specificity**
✅ Good: "Create a Python function that validates email addresses using regex"
❌ Poor: "Make something that checks emails"

### 2. **Context & Motivation**
✅ Good: "Format output as JSON because it will be parsed by another system"
❌ Poor: "Use JSON format"

### 3. **Positive Instructions**
✅ Good: "Write responses in clear, professional language"
❌ Poor: "Don't use informal language"

### 4. **Examples Alignment**
Ensure any examples match your desired output exactly.

## Quick Improvements Checklist

When I evaluate your prompt, I'll check:

1. **Is the task clearly defined?**
   - Add specific requirements
   - Include success criteria
   - Define scope boundaries

2. **Does it leverage Claude's strengths?**
   - For complex tasks: Request step-by-step thinking
   - For parallel operations: Ask for simultaneous tool use
   - For code: Specify language, framework, and style preferences

3. **Are outputs well-defined?**
   - Specify format (JSON, markdown, etc.)
   - Include structure examples
   - Use XML tags for complex outputs

4. **Common issues to fix:**
   - Vague instructions → Add specifics
   - Negative phrasing → Rewrite positively
   - Missing context → Explain the "why"
   - No examples → Add 1-2 clear examples

## Example Enhancement

**Original**: "Write code"
**Improved**: "Write a Python function with type hints that processes user data. Include error handling, docstrings, and follow PEP 8 conventions. The function should validate input and return a structured response."

Provide your prompt and I'll give you targeted improvements!